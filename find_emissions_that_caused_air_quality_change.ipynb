{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Find the emission pathways that caused the change in air quality over 2015-2017 in China**\n",
    "- Per gridcell (emulator).  \n",
    "- Remove the inputs that don't predict the correct change.  \n",
    "\n",
    "##### **Steps**\n",
    "\n",
    "*Calculated on HPC using `find_emissions_that_caused_air_quality_change.py` and `find_emissions_that_caused_air_quality_change.bash`.*\n",
    "\n",
    "1. Load observations for 1 station.  \n",
    "2. Find the change in measured PM2.5 (annual-mean) and O3 (6mDM8h) concentrations for this location over 2015-2017.  \n",
    "3. Filter through predictions of all emission configurations for this location. \n",
    "4. Keep emission configurations where the prediction matchs (within 1%) the measured change in PM2.5/O3 concentrations.  \n",
    "\n",
    "*Calculated in this notebook.*\n",
    "\n",
    "5. Split by region.  \n",
    "6. Compare to bottom-up estimates.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tabula\n",
    "import joblib\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "params = {\n",
    "    'text.latex.preamble': ['\\\\usepackage{gensymb}'],\n",
    "    'axes.grid': False,\n",
    "    'savefig.dpi': 700,\n",
    "    'font.size': 12,\n",
    "    'text.usetex': True,\n",
    "    'figure.figsize': [5, 5],\n",
    "    'font.family': 'serif',\n",
    "}\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### bottom up emissions 2015-2017 - Zheng et al., 2018 ACP\n",
    "df = tabula.read_pdf('/nfs/b0122/Users/earlacoa/paper_aia_china/emulator_annual/zheng2018.pdf', pages=7)\n",
    "df.drop(columns=['Unnamed: 2', 'Unnamed: 4', 'Unnamed: 9', 'c'], inplace=True)\n",
    "df.columns = ['sector', 'so2', 'nox', 'nmvoc', 'nh3', 'co', 'tsp', 'pm10', 'pm25', 'bc', 'oc', 'co2']\n",
    "\n",
    "df_2010 = df.iloc[0:7].copy()\n",
    "df_2011 = df.iloc[7:14].copy()\n",
    "df_2012 = df.iloc[14:21].copy()\n",
    "df_2013 = df.iloc[21:28].copy()\n",
    "df_2014 = df.iloc[28:35].copy()\n",
    "df_2015 = df.iloc[35:42].copy()\n",
    "df_2016 = df.iloc[42:49].copy()\n",
    "df_2017 = df.iloc[49:56].copy()\n",
    "\n",
    "df_2010.set_index('sector', inplace=True)\n",
    "df_2011.set_index('sector', inplace=True)\n",
    "df_2012.set_index('sector', inplace=True)\n",
    "df_2013.set_index('sector', inplace=True)\n",
    "df_2014.set_index('sector', inplace=True)\n",
    "df_2015.set_index('sector', inplace=True)\n",
    "df_2016.set_index('sector', inplace=True)\n",
    "df_2017.set_index('sector', inplace=True)\n",
    "\n",
    "df_2010 = df_2010.astype('float32').copy()\n",
    "df_2011 = df_2011.astype('float32').copy()\n",
    "df_2012 = df_2012.astype('float32').copy()\n",
    "df_2013 = df_2013.astype('float32').copy()\n",
    "df_2014 = df_2014.astype('float32').copy()\n",
    "df_2015 = df_2015.astype('float32').copy()\n",
    "df_2016 = df_2016.astype('float32').copy()\n",
    "df_2017 = df_2017.astype('float32').copy()\n",
    "\n",
    "df_diff = ((100 * df_2017 / df_2015) - 100).copy()\n",
    "df_diff.drop(['2015', '2017'], inplace=True)\n",
    "df_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_files = glob.glob('/nfs/a68/earlacoa/china_measurements_corrected/*.nc')\n",
    "\n",
    "df_obs = pd.read_csv(\n",
    "    '/nfs/a68/earlacoa/china_measurements_corrected/df_obs_o3_6mDM8h_ppb_PM2_5_DRY.csv',\n",
    "    index_col='datetime',\n",
    "    parse_dates=True\n",
    ")\n",
    "df_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['o3_6mDM8h_ppb', 'PM2_5_DRY']\n",
    "\n",
    "path_predictions = '/nfs/b0122/Users/earlacoa/paper_aia_china/emulator_annual/predictions'\n",
    "\n",
    "matrix_stacked = np.array(np.meshgrid(\n",
    "    np.linspace(0.3, 1.3, 6), # np.linspace(0.2, 1.3, 12) for 10% intervals\n",
    "    np.linspace(0.3, 1.3, 6), # np.linspace(0.3, 1.3, 6) for 20% intervals\n",
    "    np.linspace(0.3, 1.3, 6), # removing edges of parameter space 0.0, 0.1, 1.4, 1.5\n",
    "    np.linspace(0.3, 1.3, 6),\n",
    "    np.linspace(0.3, 1.3, 6)\n",
    ")).T.reshape(-1, 5)\n",
    "\n",
    "obs_change_abs = {}\n",
    "obs_change_per = {}\n",
    "baselines = {}\n",
    "targets = {}\n",
    "station_diffs_abs = {}\n",
    "station_diffs_per = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in outputs:\n",
    "    for obs_file in obs_files:\n",
    "        station_id = obs_file[47:-3]\n",
    "        lat = df_obs.loc[df_obs.station_id == station_id].station_lat.unique()[0]\n",
    "        lon = df_obs.loc[df_obs.station_id == station_id].station_lon.unique()[0]\n",
    "        \n",
    "        change_per = 100 * ((df_obs.loc[df_obs.station_id == station_id][output]['2017'].values[0] / \\\n",
    "                             df_obs.loc[df_obs.station_id == station_id][output]['2015'].values[0]) - 1)\n",
    "        change_abs = df_obs.loc[df_obs.station_id == station_id][output]['2017'].values[0] - \\\n",
    "                     df_obs.loc[df_obs.station_id == station_id][output]['2015'].values[0]\n",
    "\n",
    "        obs_change_abs.update({f'{station_id}_{output}': change_abs})\n",
    "        obs_change_per.update({f'{station_id}_{output}': change_per})\n",
    "\n",
    "        if output == 'o3_6mDM8h_ppb':\n",
    "            emulator_output = 'o3_6mDM8h'\n",
    "        else:\n",
    "            emulator_output = output\n",
    "            \n",
    "        with xr.open_dataset(\n",
    "            f'{path_predictions}/{emulator_output}/ds_RES1.0_IND1.0_TRA1.0_AGR1.0_ENE1.0_{emulator_output}_popgrid_0.25deg.nc'\n",
    "        )[emulator_output] as ds:\n",
    "            baseline = ds.sel(lat=lat, method='nearest').sel(lon=lon, method='nearest').values\n",
    "                    \n",
    "        baselines.update({f'{station_id}_{output}': baseline})\n",
    "\n",
    "        target_abs = baseline + change_abs\n",
    "        target_per = baseline * (1 + (change_per / 100))\n",
    "        target = np.mean([target_abs, target_per])\n",
    "        targets.update({f'{station_id}_{output}': target})\n",
    "        \n",
    "        target_diffs_abs = {}\n",
    "        target_diffs_per = {}\n",
    "        \n",
    "        for matrix in matrix_stacked:\n",
    "            inputs = matrix.reshape(-1, 5)        \n",
    "            filename = f'RES{inputs[0][0]:.1f}_IND{inputs[0][1]:.1f}_TRA{inputs[0][2]:.1f}_AGR{inputs[0][3]:.1f}_ENE{inputs[0][4]:.1f}'\n",
    "            with xr.open_dataset(\n",
    "                f'{path_predictions}/{emulator_output}/ds_{filename}_{emulator_output}_popgrid_0.25deg.nc'\n",
    "            )[emulator_output] as ds:\n",
    "                prediction = ds.sel(lat=lat, method='nearest').sel(lon=lon, method='nearest').values\n",
    "\n",
    "            target_diff_abs = targets[f'{station_id}_{output}'] - prediction\n",
    "            target_diff_per = (100 * (prediction / targets[f'{station_id}_{output}'])) - 100\n",
    "            \n",
    "            if abs(target_diff_per) < 1: # +/- 1% of target\n",
    "                target_diffs_abs.update({filename: target_diff_abs})\n",
    "                target_diffs_per.update({filename: target_diff_per})\n",
    "\n",
    "        station_diffs_abs.update({f'{station_id}_{output}': target_diffs_abs})\n",
    "        station_diffs_per.update({f'{station_id}_{output}': target_diffs_per})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in [key for key in targets.keys()]:\n",
    "#     if np.isnan(targets[key]):\n",
    "#         del targets[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_diffs = {}\n",
    "for key in [key for key in targets.keys()]:\n",
    "    target_diffs.update({key: targets[key] - baselines[key]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in list(set([key for key in emulators.keys()]) - set([key for key in targets.keys()])):\n",
    "#     del emulators[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [list(station_diffs_per[station].keys())for station in station_diffs_per.keys()]\n",
    "keys_flatten = [item for sublist in keys for item in sublist]\n",
    "keys_unique = {}\n",
    "for key in keys_flatten:\n",
    "    if key not in keys_unique:\n",
    "        keys_unique.update({key: 1})\n",
    "    elif key in keys_unique:\n",
    "        keys_unique.update({key: keys_unique[key] + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/nfs/b0122/Users/earlacoa/paper_aia_china/emulator_annual/find_emissions_that_match_change_air_quality/2015-2017_20percentintervals'\n",
    "\n",
    "joblib.dump(targets, f'{path}/targets.joblib')\n",
    "joblib.dump(baselines, f'{path}/baselines.joblib')\n",
    "joblib.dump(target_diffs, f'{path}/target_diffs.joblib')\n",
    "joblib.dump(obs_change_abs, f'{path}/obs_change_abs.joblib')\n",
    "joblib.dump(obs_change_per, f'{path}/obs_change_per.joblib')\n",
    "joblib.dump(keys_unique, f'{path}/keys_unique.joblib')\n",
    "joblib.dump(station_diffs_per, f'{path}/station_diffs_per_1percent.joblib')\n",
    "joblib.dump(station_diffs_abs, f'{path}/station_diffs_abs_1percent.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/nfs/b0122/Users/earlacoa/paper_aia_china/emulator_annual/find_emissions_that_match_change_air_quality'\n",
    "\n",
    "files_targets = sorted(glob.glob(f'{path}/2015-2017_20percentintervals/targets*.joblib'))\n",
    "files_baselines = sorted(glob.glob(f'{path}/2015-2017_20percentintervals/baselines*.joblib'))\n",
    "files_target_diffs = sorted(glob.glob(f'{path}/2015-2017_20percentintervals/target_diffs*.joblib'))\n",
    "files_station_diffs_per = sorted(glob.glob(f'{path}/2015-2017_20percentintervals/station_diffs_per*.joblib'))\n",
    "files_station_diffs_abs = sorted(glob.glob(f'{path}/2015-2017_20percentintervals/station_diffs_abs*.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = {}\n",
    "baselines = {}\n",
    "target_diffs = {}\n",
    "station_diffs_per = {}\n",
    "station_diffs_abs = {}\n",
    "\n",
    "for index in range(len(files_targets)):\n",
    "    if 'PM2_5_DRY' in files_target_diffs[index]:\n",
    "        output = 'PM2_5_DRY'\n",
    "    elif 'o3_6mDM8h' in files_target_diffs[index]:\n",
    "        output = 'o3_6mDM8h'\n",
    "        \n",
    "    station_id = files_target_diffs[index][-12:-7]\n",
    "    \n",
    "    target = joblib.load(files_targets[index])\n",
    "    baseline = joblib.load(files_baselines[index])\n",
    "    target_diff = joblib.load(files_target_diffs[index])\n",
    "    station_diff_per = joblib.load(files_station_diffs_per[index])\n",
    "    station_diff_abs = joblib.load(files_station_diffs_abs[index])\n",
    "    \n",
    "    targets.update({f'{station_id}_{output}': target[[key for key in target.keys()][0]]})\n",
    "    baselines.update({f'{station_id}_{output}': baseline[[key for key in baseline.keys()][0]]})\n",
    "    target_diffs.update({f'{station_id}_{output}': target_diff[[key for key in target_diff.keys()][0]]})\n",
    "    station_diffs_per.update({f'{station_id}_{output}': station_diff_per[[key for key in station_diff_per.keys()][0]]})\n",
    "    station_diffs_abs.update({f'{station_id}_{output}': station_diff_abs[[key for key in station_diff_abs.keys()][0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targets\n",
    "#baselines\n",
    "#target_diffs\n",
    "#station_diffs_per\n",
    "#station_diffs_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(targets, f'{path}/2015-2017_20percentintervals/targets.joblib')\n",
    "joblib.dump(baselines, f'{path}/2015-2017_20percentintervals/baselines.joblib')\n",
    "joblib.dump(target_diffs, f'{path}/2015-2017_20percentintervals/target_diffs.joblib')\n",
    "joblib.dump(station_diffs_per, f'{path}/2015-2017_20percentintervals/station_diffs_per.joblib')\n",
    "joblib.dump(station_diffs_abs, f'{path}/2015-2017_20percentintervals/station_diffs_abs.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/nfs/b0122/Users/earlacoa/paper_aia_china/emulator_annual/find_emissions_that_match_change_air_quality'\n",
    "\n",
    "targets = joblib.load(f'{path}/2015-2017_20percentintervals/targets.joblib')\n",
    "baselines = joblib.load(f'{path}/2015-2017_20percentintervals/baselines.joblib')\n",
    "target_diffs = joblib.load(f'{path}/2015-2017_20percentintervals/target_diffs.joblib')\n",
    "station_diffs_per = joblib.load(f'{path}/2015-2017_20percentintervals/station_diffs_per.joblib')\n",
    "station_diffs_abs = joblib.load(f'{path}/2015-2017_20percentintervals/station_diffs_abs.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_provinces = {\n",
    "    'Beijing': 'North China',\n",
    "    'Tianjin': 'North China',\n",
    "    'Hebei': 'North China',\n",
    "    'Shanxi': 'North China',\n",
    "    'Nei Mongol': 'North China',\n",
    "    'Liaoning': 'North East China',\n",
    "    'Jilin': 'North East China',\n",
    "    'Heilongjiang': 'North East China',\n",
    "    'Shanghai': 'East China',\n",
    "    'Jiangsu': 'East China',\n",
    "    'Zhejiang': 'East China', \n",
    "    'Anhui': 'East China', \n",
    "    'Fujian': 'East China', \n",
    "    'Jiangxi': 'East China', \n",
    "    'Shandong': 'East China',\n",
    "    'Taiwan': 'East China',\n",
    "    'Henan': 'South Central China',\n",
    "    'Hubei': 'South Central China',\n",
    "    'Hunan': 'South Central China',\n",
    "    'Guangdong': 'South Central China',\n",
    "    'Guangxi': 'South Central China',\n",
    "    'Hainan': 'South Central China',\n",
    "    'Hong Kong': 'South Central China',\n",
    "    'Macao': 'South Central China',\n",
    "    'Chongqing': 'South West China',\n",
    "    'Sichuan': 'South West China',\n",
    "    'Guizhou': 'South West China',\n",
    "    'Yunnan': 'South West China',\n",
    "    'Xizang': 'South West China',\n",
    "    'Shaanxi': 'North West China',\n",
    "    'Gansu': 'North West China', \n",
    "    'Qinghai': 'North West China',\n",
    "    'Ningxia Hui': 'North West China',\n",
    "    'Xinjiang Uygur': 'North West China'\n",
    "}\n",
    "\n",
    "gba_prefectures = ['Dongguan', 'Foshan', 'Guangzhou', 'Huizhou', 'Jiangmen', 'Shenzhen', 'Zhaoqing', 'Zhongshan', 'Zhuhai', 'Hong Kong', 'Macao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_prefectures_china = gpd.read_file('/nfs/a68/earlacoa/shapefiles/china/gadm36_CHN_3.shp')\n",
    "gdf_prefectures_hongkong = gpd.read_file('/nfs/a68/earlacoa/shapefiles/hongkong/gadm36_HKG_1.shp')\n",
    "gdf_prefectures_macao = gpd.read_file('/nfs/a68/earlacoa/shapefiles/macao/gadm36_MAC_2.shp')\n",
    "\n",
    "list_prefectures_gba = []\n",
    "for gba_prefecture in gba_prefectures:\n",
    "    list_prefectures_gba.append(gdf_prefectures_china.loc[gdf_prefectures_china.NAME_2 == gba_prefecture])\n",
    "\n",
    "\n",
    "list_prefectures_gba.append(gdf_prefectures_hongkong)\n",
    "list_prefectures_gba.append(gdf_prefectures_macao)\n",
    "    \n",
    "gdf_prefectures_gba = pd.concat(list_prefectures_gba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefectures = {\n",
    "    'North China': [],\n",
    "    'North East China': [],\n",
    "    'East China': [],\n",
    "    'South Central China': [],\n",
    "    'South West China': [],\n",
    "    'North West China': [],\n",
    "}\n",
    "for prefecture in gdf_prefectures_china.NAME_2.unique():\n",
    "    province = gdf_prefectures_china.loc[gdf_prefectures_china.NAME_2 == prefecture].NAME_1.unique()[0]\n",
    "    region = regional_provinces[province]\n",
    "    prefectures[region].append(prefecture)\n",
    "    \n",
    "\n",
    "for region, prefecture_list in prefectures.items():\n",
    "    prefecture_list = list(set(prefecture_list))\n",
    "    prefectures.update({region: prefecture_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_prefectures = {}\n",
    "for region, prefecture_list in prefectures.items():\n",
    "    prefecture_list = list(set(prefecture_list))\n",
    "    for prefecture in prefecture_list:\n",
    "        regional_prefectures.update({prefecture: region})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_china_north = gpd.read_file('/nfs/a68/earlacoa/shapefiles/china/CHN_north.shp')\n",
    "gdf_china_north_east = gpd.read_file('/nfs/a68/earlacoa/shapefiles/china/CHN_north_east.shp')\n",
    "gdf_china_east = gpd.read_file('/nfs/a68/earlacoa/shapefiles/china/CHN_east.shp')\n",
    "gdf_china_south_central = gpd.read_file('/nfs/a68/earlacoa/shapefiles/china/CHN_south_central.shp')\n",
    "gdf_china_south_west = gpd.read_file('/nfs/a68/earlacoa/shapefiles/china/CHN_south_west.shp')\n",
    "gdf_china_north_west = gpd.read_file('/nfs/a68/earlacoa/shapefiles/china/CHN_north_west.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_lats = {}\n",
    "obs_lons = {}\n",
    "obs_prefectures = {}\n",
    "obs_regions = {}\n",
    "obs_gba = {}\n",
    "  \n",
    "for obs_file in obs_files:\n",
    "    with xr.open_dataset(obs_file) as ds:\n",
    "        station_id = ds.station\n",
    "        if '.nc' in station_id:\n",
    "            station_id = station_id[:-3]\n",
    "            \n",
    "        obs_lats.update({station_id: ds.lat})\n",
    "        obs_lons.update({station_id: ds.lon})\n",
    "        obs_prefectures.update({station_id: ds.city})\n",
    "        \n",
    "        if gpd.GeoSeries(Point(ds.lon, ds.lat)).within(gdf_china_north).values[0]:\n",
    "            region = 'North China'\n",
    "        elif gpd.GeoSeries(Point(ds.lon, ds.lat)).within(gdf_china_north_east).values[0]:\n",
    "            region = 'North East China'\n",
    "        elif gpd.GeoSeries(Point(ds.lon, ds.lat)).within(gdf_china_east).values[0]:\n",
    "            region = 'East China'\n",
    "        elif gpd.GeoSeries(Point(ds.lon, ds.lat)).within(gdf_china_south_central).values[0]:\n",
    "            region = 'South Central China'\n",
    "        elif gpd.GeoSeries(Point(ds.lon, ds.lat)).within(gdf_china_south_west).values[0]:\n",
    "            region = 'South West China'\n",
    "        elif gpd.GeoSeries(Point(ds.lon, ds.lat)).within(gdf_china_north_west).values[0]:\n",
    "            region = 'North West China'\n",
    "            \n",
    "        obs_regions.update({station_id: region})\n",
    "        if (ds.city in gba_prefectures) or (ds.city == 'Hong Kong') or (ds.city == 'Macao'):\n",
    "            obs_gba.update({station_id: True})\n",
    "        else:\n",
    "            obs_gba.update({station_id: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_lats = {}\n",
    "# obs_lons = {}\n",
    "# obs_prefectures = {}\n",
    "# obs_regions = {}\n",
    "# obs_gba = {}\n",
    "  \n",
    "# for obs_file in obs_files:\n",
    "#     with xr.open_dataset(obs_file) as ds:\n",
    "#         station_id = ds.station\n",
    "#         if '.nc' in station_id:\n",
    "#             station_id = station_id[:-3]\n",
    "            \n",
    "#         obs_lats.update({station_id: ds.lat})\n",
    "#         obs_lons.update({station_id: ds.lon})\n",
    "#         obs_prefectures.update({station_id: ds.city})\n",
    "#         if ds.city == 'Lhasa Googut':\n",
    "#             region = regional_prefectures['Lhasa']\n",
    "#         elif ds.city == 'Huaian':\n",
    "#             region = regional_prefectures[\"Huai'an\"]\n",
    "#         elif ds.city == 'Urumqi':\n",
    "#             region = 'North West China'\n",
    "#         elif ds.city == 'Ordos City':\n",
    "#             region = regional_prefectures[\"Ordos\"]\n",
    "#         elif ds.city == 'Yingkou':\n",
    "#             region = 'North East China'\n",
    "#         elif ds.city == 'Sunlight':\n",
    "#             region = 'East China'\n",
    "#         elif ds.city == 'Iris':\n",
    "#             region = 'South Central China'\n",
    "#         elif ds.city == 'chicken':\n",
    "#             region = 'South Central China'\n",
    "#         elif ds.city == 'Mudan River':\n",
    "#             region = 'North West China'\n",
    "#         elif ds.city == 'Maanshan':\n",
    "#             region = 'East China'\n",
    "#         elif ds.city == 'North Sea':\n",
    "#             region = 'South Central China'\n",
    "#         elif ds.city == 'Yanan':\n",
    "#             region = 'North China'\n",
    "#         elif ds.city == 'Korla':\n",
    "#             region = 'North West China'\n",
    "#         elif ds.city == 'Shouguang':\n",
    "#             region = 'East China'\n",
    "#         elif ds.city == 'Zhang Qiu':\n",
    "#             region = 'East China'\n",
    "#         elif ds.city == 'Jimo':\n",
    "#             region = 'East China'\n",
    "#         elif ds.city == 'Jiaonan':\n",
    "#             region = 'East China'\n",
    "#         elif ds.city == 'Jiaozhou':\n",
    "#             region = 'East China'\n",
    "#         elif ds.city == 'Laixi':\n",
    "#             region = 'East China'\n",
    "#         elif ds.city == 'Pingdu':\n",
    "#             region = 'East China'\n",
    "#         elif ds.city == 'Penglai':\n",
    "#             region = 'East China'\n",
    "#         else:\n",
    "#             region = regional_prefectures[ds.city]\n",
    "            \n",
    "#         obs_regions.update({station_id: region})\n",
    "#         if (ds.city in gba_prefectures) or (ds.city == 'Hong Kong') or (ds.city == 'Macao'):\n",
    "#             obs_gba.update({station_id: True})\n",
    "#         else:\n",
    "#             obs_gba.update({station_id: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['China', 'GBA', 'North China', 'North East China', 'East China', 'South Central China', 'South West China', 'North West China']\n",
    "region_stations = {key: [] for key in regions}\n",
    "\n",
    "for station_id, station_region in obs_regions.items():\n",
    "    region_stations['China'].append(station_id)\n",
    "    region_stations[station_region].append(station_id)\n",
    "    \n",
    "    if obs_gba[station_id] == True:\n",
    "        region_stations['GBA'].append(station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_targets = {}\n",
    "regional_baselines = {}\n",
    "regional_target_diffs = {}\n",
    "regional_station_diffs_per = {}\n",
    "regional_station_diffs_abs = {}\n",
    "\n",
    "for region in regions:\n",
    "    regional_targets.update({region: dict((key, value) for key, value in targets.items() if key[0:5] in region_stations[region])})\n",
    "    regional_baselines.update({region: dict((key, value) for key, value in baselines.items() if key[0:5] in region_stations[region])})\n",
    "    regional_target_diffs.update({region: dict((key, value) for key, value in target_diffs.items() if key[0:5] in region_stations[region])})\n",
    "    regional_station_diffs_per.update({region: dict((key, value) for key, value in station_diffs_per.items() if key[0:5] in region_stations[region])})\n",
    "    regional_station_diffs_abs.update({region: dict((key, value) for key, value in station_diffs_abs.items() if key[0:5] in region_stations[region])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['PM2_5_DRY', 'o3_6mDM8h', 'combined']\n",
    "\n",
    "regional_keys_unique_sorted = {}\n",
    "regional_target_diffs_under0p5 = {}\n",
    "\n",
    "for output in outputs:\n",
    "    for region in regions:\n",
    "        if output == 'combined':\n",
    "            keys = [list(regional_station_diffs_per[region][stationid_output].keys()) for stationid_output in regional_station_diffs_per[region].keys()]\n",
    "        else:\n",
    "            keys = [list(regional_station_diffs_per[region][stationid_output].keys()) for stationid_output in regional_station_diffs_per[region].keys() if output in stationid_output]\n",
    "            \n",
    "        keys_flatten = [item for sublist in keys for item in sublist]\n",
    "\n",
    "        keys_unique = {}\n",
    "        for key in keys_flatten:\n",
    "            if key not in keys_unique:\n",
    "                keys_unique.update({key: 1})\n",
    "            elif key in keys_unique:\n",
    "                keys_unique.update({key: keys_unique[key] + 1})\n",
    "\n",
    "\n",
    "        keys_unique_sorted = {key: value for key, value in sorted(keys_unique.items(), key=lambda item: item[1], reverse=True)}\n",
    "        regional_keys_unique_sorted.update({f'{region}_{output}': keys_unique_sorted})\n",
    "\n",
    "        target_diffs_under0p5 = {}\n",
    "        for key, value in regional_target_diffs[region].items():\n",
    "            if abs(value) < 0.5:\n",
    "                target_diffs_under0p5.update({key: value})\n",
    "\n",
    "\n",
    "        regional_target_diffs_under0p5.update({f'{region}_{output}': target_diffs_under0p5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_stations = {}\n",
    "number_of_emission_configurations = {}\n",
    "first_emission_configuration_keys = {}\n",
    "second_emission_configuration_keys = {}\n",
    "third_emission_configuration_keys = {}\n",
    "first_emission_configuration_values = {}\n",
    "second_emission_configuration_values = {}\n",
    "third_emission_configuration_values = {}\n",
    "number_of_stations_with_target_diff_under_0p5 = {}\n",
    "\n",
    "for output in outputs:\n",
    "    for region in regional_targets.keys():\n",
    "        number_of_stations.update({f'{region}_{output}': len(regional_targets[region].keys())})\n",
    "        number_of_emission_configurations.update({f'{region}_{output}': len(regional_keys_unique_sorted[f'{region}_{output}'].keys())})\n",
    "        \n",
    "        top3_emission_configurations = list(islice(regional_keys_unique_sorted[f'{region}_{output}'].items(), 3))\n",
    "        \n",
    "        first_emission_configuration_keys.update({f'{region}_{output}': top3_emission_configurations[0][0]})\n",
    "        second_emission_configuration_keys.update({f'{region}_{output}': top3_emission_configurations[1][0]})\n",
    "        third_emission_configuration_keys.update({f'{region}_{output}': top3_emission_configurations[2][0]})\n",
    "        first_emission_configuration_values.update({f'{region}_{output}': top3_emission_configurations[0][1]})\n",
    "        second_emission_configuration_values.update({f'{region}_{output}': top3_emission_configurations[1][1]})\n",
    "        third_emission_configuration_values.update({f'{region}_{output}': top3_emission_configurations[2][1]})\n",
    "        number_of_stations_with_target_diff_under_0p5.update({f'{region}_{output}': len(regional_target_diffs_under0p5[f'{region}_{output}'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions = pd.concat([\n",
    "    pd.Series(number_of_stations, name='Stations'),\n",
    "    pd.Series(number_of_emission_configurations, name='Possible Emission Configurations'),\n",
    "    pd.Series(first_emission_configuration_keys, name='First Emission Configuration - Key'),\n",
    "    pd.Series(first_emission_configuration_values, name='First Emission Configuration - Value'),\n",
    "    pd.Series(second_emission_configuration_keys, name='Second Emission Configuration - Key'),\n",
    "    pd.Series(second_emission_configuration_values, name='Second Emission Configuration - Value'),\n",
    "    pd.Series(third_emission_configuration_keys, name='Third Emission Configuration - Key'),\n",
    "    pd.Series(third_emission_configuration_values, name='Third Emission Configuration - Value'),\n",
    "    pd.Series(number_of_stations_with_target_diff_under_0p5, name='Stations with trend size under 0.5 ugm-3')\n",
    "], axis=1)\n",
    "df_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions.to_csv(f'{path}/df_regions.csv')\n",
    "joblib.dump(regional_keys_unique_sorted, f'{path}/regional_keys_unique_sorted.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions = pd.read_csv(f'{path}/df_regions.csv')\n",
    "regional_keys_unique_sorted = joblib.load(f'{path}/regional_keys_unique_sorted.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_boxplot(index, values, sector, bottomup):\n",
    "    ax = fig.add_subplot(gs[index])\n",
    "    ax.set_facecolor('whitesmoke')\n",
    "    plt.ylim([0.0, 1.5])\n",
    "    plt.yticks(np.arange(0, 1.75, 0.25))\n",
    "    ax.set_yticklabels(np.arange(0, 175, 25))\n",
    "    plt.yticks(fontsize=14)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    if index == 0:\n",
    "        plt.ylabel('Emission change ({\\%})', fontsize=14)\n",
    "    else:\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "    plt.title(sector)\n",
    "    color1 = '#c7eae5'\n",
    "    color2 = '#01665e'\n",
    "    plt.boxplot(\n",
    "        values, \n",
    "        patch_artist=True,\n",
    "        boxprops={'facecolor': color1, 'color': color2, 'linewidth': 1.5},\n",
    "        capprops={'color': color2, 'linewidth': 1.5},\n",
    "        whiskerprops={'color': color2, 'linewidth': 1.5},\n",
    "        flierprops={'color': color2, 'markeredgecolor': color2, 'linewidth': 1.5},\n",
    "        medianprops={'color': color2, 'linewidth': 1.5},\n",
    "        showmeans=True,\n",
    "        meanprops={'markeredgecolor': color2, 'color': color2},\n",
    "        showfliers=False,\n",
    "        whis=(5, 95),\n",
    "        zorder=1\n",
    "    )\n",
    "    plt.scatter(1, bottomup, color='#8c510a', zorder=2, marker='*')\n",
    "    plt.annotate(r'\\textbf{(' + chr(97 + index) + ')}', xy=(0, 1.05), xycoords='axes fraction', fontsize=14, weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhang2018_bottomup20152017_allspecies_res = 1 + (df_diff.loc['Residential'][['so2', 'nox', 'nmvoc', 'nh3', 'co', 'pm25', 'bc', 'oc', 'pm10']].mean() / 100)\n",
    "zhang2018_bottomup20152017_allspecies_ind = 1 + (df_diff.loc['Industry'][['so2', 'nox', 'nmvoc', 'nh3', 'co', 'pm25', 'bc', 'oc', 'pm10']].mean() / 100)\n",
    "zhang2018_bottomup20152017_allspecies_tra = 1 + (df_diff.loc['Transportation'][['so2', 'nox', 'nmvoc', 'nh3', 'co', 'pm25', 'bc', 'oc', 'pm10']].mean() / 100)\n",
    "zhang2018_bottomup20152017_allspecies_agr = 1 + (df_diff.loc['Agriculture'][['so2', 'nox', 'nmvoc', 'nh3', 'co', 'pm25', 'bc', 'oc', 'pm10']].mean() / 100)\n",
    "zhang2018_bottomup20152017_allspecies_ene = 1 + (df_diff.loc['Power'][['so2', 'nox', 'nmvoc', 'nh3', 'co', 'pm25', 'bc', 'oc', 'pm10']].mean() / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'combined'\n",
    "\n",
    "factors_all = {}\n",
    "\n",
    "for region in regions:\n",
    "    factors_res = {}\n",
    "    factors_ind = {}\n",
    "    factors_tra = {}\n",
    "    factors_agr = {}\n",
    "    factors_ene = {}\n",
    "    for index, items in enumerate(list(itertools.islice(regional_keys_unique_sorted[f'{region}_{output}'].items(), 100))):\n",
    "        factor_res, factor_ind, factor_tra, factor_agr, factor_ene = [float(item) for item in re.findall('\\d+\\.\\d+',  items[0])]\n",
    "        factors_res.update({index: factor_res})\n",
    "        factors_ind.update({index: factor_ind})\n",
    "        factors_tra.update({index: factor_tra})\n",
    "        factors_agr.update({index: factor_agr})\n",
    "        factors_ene.update({index: factor_ene})\n",
    "        \n",
    "    factors_all.update({f'{region}_RES': factors_res})\n",
    "    factors_all.update({f'{region}_IND': factors_ind})\n",
    "    factors_all.update({f'{region}_TRA': factors_tra})\n",
    "    factors_all.update({f'{region}_AGR': factors_agr})\n",
    "    factors_all.update({f'{region}_ENE': factors_ene})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(15, 3))\n",
    "gs = gridspec.GridSpec(1, 5)\n",
    "\n",
    "region = 'China'\n",
    "\n",
    "make_boxplot(0, factors_all[f'{region}_RES'].values(), 'RES', zhang2018_bottomup20152017_allspecies_res)\n",
    "make_boxplot(1, factors_all[f'{region}_IND'].values(), 'IND', zhang2018_bottomup20152017_allspecies_ind)\n",
    "make_boxplot(2, factors_all[f'{region}_TRA'].values(), 'TRA', zhang2018_bottomup20152017_allspecies_tra)\n",
    "make_boxplot(3, factors_all[f'{region}_AGR'].values(), 'AGR', zhang2018_bottomup20152017_allspecies_agr)\n",
    "make_boxplot(4, factors_all[f'{region}_ENE'].values(), 'ENE', zhang2018_bottomup20152017_allspecies_ene)\n",
    "\n",
    "gs.tight_layout(fig, rect=[0, 0, 0.65, 0.85])\n",
    "\n",
    "plt.annotate(r'\\textbf{$\\Delta$: Top-down from emulators}', xy=(-4.0, -0.15), xycoords='axes fraction', fontsize=14, color='#01665e')\n",
    "plt.annotate(r'\\textbf{$\\star$: Bottom-up from Zheng et al., (2018)}', xy=(-2.0, -0.15), xycoords='axes fraction', fontsize=14, color='#8c510a')\n",
    "\n",
    "#plt.savefig(f'/nfs/b0122/Users/earlacoa/png/paper_aia_emulator_annual/emission_factors_boxplot_top100_{region}_{output}.png', dpi=700, alpha=True, bbox_inches='tight')\n",
    "#plt.savefig(f'/nfs/b0122/Users/earlacoa/png/paper_aia_emulator_annual/emission_factors_boxplot_top100_{region}_{output}.eps', format='eps', dpi=700, alpha=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines_mean = {}\n",
    "targets_mean = {}\n",
    "target_diffs_mean = {}\n",
    "\n",
    "for output in ['PM2_5_DRY', 'o3_6mDM8h']:\n",
    "    for region in regions:\n",
    "        values = []\n",
    "        for key, value in regional_baselines[region].items():\n",
    "            if output in key and value is not np.nan:\n",
    "                values.append(value)\n",
    "\n",
    "        values = np.array(values)\n",
    "        baselines_mean.update({f'{region}_{output}': np.nanmean(values)})\n",
    "\n",
    "        values = []\n",
    "        for key, value in regional_targets[region].items():\n",
    "            if output in key and value is not np.nan:\n",
    "                values.append(value)\n",
    "\n",
    "        values = np.array(values)\n",
    "        targets_mean.update({f'{region}_{output}': np.nanmean(values)})\n",
    "\n",
    "        values = []\n",
    "        for key, value in regional_target_diffs[region].items():\n",
    "            if output in key and value is not np.nan:\n",
    "                values.append(value)\n",
    "\n",
    "        values = np.array(values)\n",
    "        target_diffs_mean.update({f'{region}_{output}': np.nanmean(values)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in ['PM2_5_DRY', 'o3_6mDM8h']:\n",
    "    for region in regions:\n",
    "        print(region, output)\n",
    "        percent_change = (100 * ( baselines_mean[f'{region}_{output}'] + target_diffs_mean[f'{region}_{output}'] ) / baselines_mean[f'{region}_{output}']) - 100\n",
    "        print(f\"Baseline = {round(baselines_mean[f'{region}_{output}'], 1)}\")\n",
    "        print(f\"Target = {round(targets_mean[f'{region}_{output}'], 1)}\")\n",
    "        #print(f\"Absolute change = {round(target_diffs_mean[f'{region}_{output}'], 1)}\")\n",
    "        print(f\"Percentage change = {round(percent_change, 1)} %\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RES', round(zhang2018_bottomup20152017_allspecies_res, 2))\n",
    "print('IND', round(zhang2018_bottomup20152017_allspecies_ind, 2))\n",
    "print('TRA', round(zhang2018_bottomup20152017_allspecies_tra, 2))\n",
    "print('AGR', round(zhang2018_bottomup20152017_allspecies_agr, 2))\n",
    "print('ENE', round(zhang2018_bottomup20152017_allspecies_ene, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = ['RES', 'IND', 'TRA', 'AGR', 'ENE']\n",
    "color1 = '#c7eae5'\n",
    "color2 = '#01665e'\n",
    "\n",
    "for region in regions:\n",
    "    print(region)\n",
    "    for sector in sectors:\n",
    "        bp = plt.boxplot(\n",
    "            factors_all[f'{region}_{sector}'].values(), \n",
    "            patch_artist=True,\n",
    "            boxprops={'facecolor': color1, 'color': color2, 'linewidth': 1.5},\n",
    "            capprops={'color': color2, 'linewidth': 1.5},\n",
    "            whiskerprops={'color': color2, 'linewidth': 1.5},\n",
    "            flierprops={'color': color2, 'markeredgecolor': color2, 'linewidth': 1.5},\n",
    "            medianprops={'color': color2, 'linewidth': 1.5},\n",
    "            showmeans=True,\n",
    "            meanprops={'markeredgecolor': color2, 'color': color2},\n",
    "            showfliers=False,\n",
    "            whis=(5, 95),\n",
    "            zorder=1\n",
    "        )\n",
    "        print(sector, round(bp['means'][0].get_ydata()[0], 2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# health impact assessment for both top-down and bottom-up\n",
    "# rounded to the nearest 10% change in emissions\n",
    "\n",
    "ctl = 'RES1.0_IND1.0_TRA1.0_AGR1.0_ENE1.0'\n",
    "\n",
    "bottom_up_overall           = 'RES0.8_IND0.8_TRA1.0_AGR1.0_ENE0.9'\n",
    "bottom_up_contribution_res  = 'RES0.8_IND1.0_TRA1.0_AGR1.0_ENE1.0'\n",
    "bottom_up_contribution_ind  = 'RES1.0_IND0.8_TRA1.0_AGR1.0_ENE1.0'\n",
    "bottom_up_contribution_ene  = 'RES1.0_IND1.0_TRA1.0_AGR1.0_ENE0.9'\n",
    "\n",
    "top_down_overall           = 'RES0.9_IND1.0_TRA0.9_AGR0.9_ENE0.5'\n",
    "top_down_contribution_res  = 'RES0.9_IND1.0_TRA1.0_AGR1.0_ENE1.0'\n",
    "top_down_contribution_tra  = 'RES1.0_IND1.0_TRA0.9_AGR1.0_ENE1.0'\n",
    "top_down_contribution_agr  = 'RES1.0_IND1.0_TRA1.0_AGR0.9_ENE1.0'\n",
    "top_down_contribution_ene  = 'RES1.0_IND1.0_TRA1.0_AGR1.0_ENE0.5'\n",
    "\n",
    "sims = [ctl, bottom_up_overall, bottom_up_contribution_res, bottom_up_contribution_ind, bottom_up_contribution_ene, top_down_overall, top_down_contribution_res, top_down_contribution_tra, top_down_contribution_agr, top_down_contribution_ene]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/nfs/b0122/Users/earlacoa/paper_aia_china/emulator_annual/health_impact_assessments'\n",
    "outputs = ['PM2_5_DRY', 'o3_6mDM8h']\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for sim in sims:\n",
    "    for output in outputs:\n",
    "        try:\n",
    "            df = pd.read_csv(f'{path}/{output}/df_country_hia_{output}_{sim}.csv')\n",
    "            dfs.update({f'{output}_{sim}': df})\n",
    "        except:\n",
    "            FileNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff_bottom_up_overall = dfs[f'PM2_5_DRY_{bottom_up_overall}'].mort_ncdlri_mean_total.values[0] - dfs[f'PM2_5_DRY_{ctl}'].mort_ncdlri_mean_total.values[0]\n",
    "diff_bottom_up_res = dfs[f'PM2_5_DRY_{bottom_up_contribution_res}'].mort_ncdlri_mean_total.values[0] - dfs[f'PM2_5_DRY_{ctl}'].mort_ncdlri_mean_total.values[0]\n",
    "diff_bottom_up_ind = dfs[f'PM2_5_DRY_{bottom_up_contribution_ind}'].mort_ncdlri_mean_total.values[0] - dfs[f'PM2_5_DRY_{ctl}'].mort_ncdlri_mean_total.values[0]\n",
    "diff_bottom_up_ene = dfs[f'PM2_5_DRY_{bottom_up_contribution_ene}'].mort_ncdlri_mean_total.values[0] - dfs[f'PM2_5_DRY_{ctl}'].mort_ncdlri_mean_total.values[0]\n",
    "\n",
    "diff_top_down_overall = dfs[f'PM2_5_DRY_{top_down_overall}'].mort_ncdlri_mean_total.values[0] - dfs[f'PM2_5_DRY_{ctl}'].mort_ncdlri_mean_total.values[0]\n",
    "#diff_top_down_res = dfs[f'PM2_5_DRY_{top_down_contribution_res}'].mort_ncdlri_mean_total.values[0] - dfs[f'PM2_5_DRY_{ctl}'].mort_ncdlri_mean_total.values[0]\n",
    "diff_top_down_tra = dfs[f'PM2_5_DRY_{top_down_contribution_tra}'].mort_ncdlri_mean_total.values[0] - dfs[f'PM2_5_DRY_{ctl}'].mort_ncdlri_mean_total.values[0]\n",
    "#diff_top_down_agr = dfs[f'PM2_5_DRY_{top_down_contribution_agr}'].mort_ncdlri_mean_total.values[0] - dfs[f'PM2_5_DRY_{ctl}'].mort_ncdlri_mean_total.values[0]\n",
    "#diff_top_down_ene = dfs[f'PM2_5_DRY_{top_down_contribution_ene}'].mort_ncdlri_mean_total.values[0] - dfs[f'PM2_5_DRY_{ctl}'].mort_ncdlri_mean_total.values[0]\n",
    "\n",
    "#print(f'Bottom up, contribution RES = {round((100 * diff_bottom_up_res / diff_bottom_up_overall), 2)} %')\n",
    "#print(f'Bottom up, contribution IND = {round((100 * diff_bottom_up_ind / diff_bottom_up_overall), 2)} %')\n",
    "#print(f'Bottom up, contribution ENE = {round((100 * diff_bottom_up_ene / diff_bottom_up_overall), 2)} %')\n",
    "print()\n",
    "#print(f'Top down, contribution RES = {round((100 * diff_top_down_res / diff_top_down_overall), 2)} %')\n",
    "print(f'Top down, contribution TRA = {round((100 * diff_top_down_tra / diff_top_down_overall), 2)} %')\n",
    "#print(f'Top down, contribution AGR = {round((100 * diff_top_down_agr / diff_top_down_overall), 2)} %')\n",
    "#print(f'Top down, contribution ENE = {round((100 * diff_top_down_ene / diff_top_down_overall), 2)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff_bottom_up_overall = dfs[f'o3_6mDM8h_{bottom_up_overall}'].mort_copd_mean_total.values[0] - dfs[f'o3_6mDM8h_{ctl}'].mort_copd_mean_total.values[0]\n",
    "diff_bottom_up_res = dfs[f'o3_6mDM8h_{bottom_up_contribution_res}'].mort_copd_mean_total.values[0] - dfs[f'o3_6mDM8h_{ctl}'].mort_copd_mean_total.values[0]\n",
    "diff_bottom_up_ind = dfs[f'o3_6mDM8h_{bottom_up_contribution_ind}'].mort_copd_mean_total.values[0] - dfs[f'o3_6mDM8h_{ctl}'].mort_copd_mean_total.values[0]\n",
    "diff_bottom_up_ene = dfs[f'o3_6mDM8h_{bottom_up_contribution_ene}'].mort_copd_mean_total.values[0] - dfs[f'o3_6mDM8h_{ctl}'].mort_copd_mean_total.values[0]\n",
    "\n",
    "diff_top_down_overall = dfs[f'o3_6mDM8h_{top_down_overall}'].mort_copd_mean_total.values[0] - dfs[f'o3_6mDM8h_{ctl}'].mort_copd_mean_total.values[0]\n",
    "diff_top_down_res = dfs[f'o3_6mDM8h_{top_down_contribution_res}'].mort_copd_mean_total.values[0] - dfs[f'o3_6mDM8h_{ctl}'].mort_copd_mean_total.values[0]\n",
    "diff_top_down_tra = dfs[f'o3_6mDM8h_{top_down_contribution_tra}'].mort_copd_mean_total.values[0] - dfs[f'o3_6mDM8h_{ctl}'].mort_copd_mean_total.values[0]\n",
    "#diff_top_down_agr = dfs[f'o3_6mDM8h_{top_down_contribution_agr}'].mort_copd_mean_total.values[0] - dfs[f'o3_6mDM8h_{ctl}'].mort_copd_mean_total.values[0]\n",
    "diff_top_down_ene = dfs[f'o3_6mDM8h_{top_down_contribution_ene}'].mort_copd_mean_total.values[0] - dfs[f'o3_6mDM8h_{ctl}'].mort_copd_mean_total.values[0]\n",
    "\n",
    "#print(f'Bottom up, contribution RES = {round((100 * diff_bottom_up_res / diff_bottom_up_overall), 2)} %')\n",
    "#print(f'Bottom up, contribution IND = {round((100 * diff_bottom_up_ind / diff_bottom_up_overall), 2)} %')\n",
    "#print(f'Bottom up, contribution ENE = {round((100 * diff_bottom_up_ene / diff_bottom_up_overall), 2)} %')\n",
    "print()\n",
    "print(f'Top down, contribution RES = {round((100 * diff_top_down_res / diff_top_down_overall), 2)} %')\n",
    "print(f'Top down, contribution TRA = {round((100 * diff_top_down_tra / diff_top_down_overall), 2)} %')\n",
    "#print(f'Top down, contribution AGR = {round((100 * diff_top_down_agr / diff_top_down_overall), 2)} %')\n",
    "print(f'Top down, contribution ENE = {round((100 * diff_top_down_ene / diff_top_down_overall), 2)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
